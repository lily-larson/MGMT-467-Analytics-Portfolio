{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lily-larson/MGMT-467-Analytics-Portfolio/blob/main/Labs/Unit2_Lab1_GCS_BQ_Data_Quality.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGs4-47rW_cu"
      },
      "source": [
        "# MGMT 467 — Prompt-Driven Lab (with Commented Examples)\n",
        "## Kaggle ➜ Google Cloud Storage ➜ BigQuery ➜ Data Quality (DQ)\n",
        "\n",
        "**How to use this notebook**\n",
        "- Each section gives you a **Build Prompt** to paste into Gemini/Vertex AI (or Gemini in Colab).\n",
        "- Below each prompt, you’ll see a **commented example** of what a good LLM answer might look like.\n",
        "- **Do not** just uncomment and run. Use the prompt to generate your own code, then compare to the example.\n",
        "- After every step, run the **Verification Prompt**, and write the **Reflection** in Markdown.\n",
        "\n",
        "> Goal today: Download the Netflix dataset (Kaggle) → Stage on GCS → Load into BigQuery → Run DQ profiling (missingness, duplicates, outliers, anomaly flags).\n"
      ],
      "id": "NGs4-47rW_cu"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TRDlcIVW_cx"
      },
      "source": [
        "### Academic integrity & LLM usage\n",
        "- Use the prompts here to generate your own code cells.\n",
        "- Read concept notes and write the reflection answers in your own words.\n",
        "- Keep credentials out of code. Upload `kaggle.json` when asked.\n"
      ],
      "id": "6TRDlcIVW_cx"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zeKuycuW_cx"
      },
      "source": [
        "## Learning objectives\n",
        "1) Explain **why** we stage data in GCS and load it to BigQuery.  \n",
        "2) Build an **idempotent**, auditable pipeline.  \n",
        "3) Diagnose **missingness**, **duplicates**, and **outliers** and justify cleaning choices.  \n",
        "4) Connect DQ decisions to **business/ML impact**.\n"
      ],
      "id": "2zeKuycuW_cx"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVALMJZeW_cy"
      },
      "source": [
        "## 0) Environment setup — What & Why\n",
        "Authenticate Colab to Google Cloud so we can use `gcloud`, GCS, and BigQuery. Set **PROJECT_ID** and **REGION** once for consistency (cost/latency)."
      ],
      "id": "CVALMJZeW_cy"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rp3nllhFW_cy"
      },
      "source": [
        "### Build Prompt (paste to LLM)\n",
        "You are my cloud TA. Generate a single **Colab code cell** that:\n",
        "1) Authenticates to Google Cloud in Colab,  \n",
        "2) Prompts for `PROJECT_ID` via `input()` and sets `REGION=\"us-central1\"` (editable),  \n",
        "3) Exports `GOOGLE_CLOUD_PROJECT`,  \n",
        "4) Runs `gcloud config set project $GOOGLE_CLOUD_PROJECT`,  \n",
        "5) Prints both values. Add 2–3 comments explaining what/why.\n",
        "End with a comment: `# Done: Auth + Project/Region set`.\n"
      ],
      "id": "rp3nllhFW_cy"
    },
    {
      "cell_type": "code",
      "source": [
        "# Authenticate to Google Cloud in Colab\n",
        "# This will prompt you to log in and select a project\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Set your Google Cloud Project ID and Region\n",
        "# PROJECT_ID: Your unique GCP project identifier\n",
        "# REGION: Google Cloud region for resources\n",
        "import os\n",
        "PROJECT_ID = input(\"Enter your GCP Project ID: \").strip()\n",
        "REGION = \"us-central1\"  # keep consistent; change if instructed\n",
        "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
        "os.environ[\"REGION\"] = REGION\n",
        "os.environ[\"JOB_VALIDATION_MODE\"] = \"DISABLED\"\n",
        "os.environ[\"TF_VALIDATION_MODE\"] = \"DISABLED\"\n",
        "\n",
        "# Set the gcloud config to your project\n",
        "# This ensures subsequent gcloud commands use your project\n",
        "!gcloud config set project $GOOGLE_CLOUD_PROJECT\n",
        "\n",
        "# Print the set values for verification\n",
        "print(\"Project:\", PROJECT_ID, \"| Region:\", REGION)\n",
        "\n",
        "# Done: Auth + Project/Region set"
      ],
      "metadata": {
        "id": "GsTFA-1ZYBbd"
      },
      "id": "GsTFA-1ZYBbd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-2fJwLbW_cz"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — Auth + Project/Region (commented; write your own cell using the prompt)\n",
        "# # from google.colab import auth\n",
        "# # auth.authenticate_user()\n",
        "# #\n",
        "# # import os\n",
        "# # PROJECT_ID = input(\"Enter your GCP Project ID: \").strip()\n",
        "# # REGION = \"us-central1\"  # keep consistent; change if instructed\n",
        "# # os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
        "# # print(\"Project:\", PROJECT_ID, \"| Region:\", REGION)\n",
        "# #\n",
        "# # # Set active project for gcloud/BigQuery CLI\n",
        "# # !gcloud config set project $GOOGLE_CLOUD_PROJECT\n",
        "# # !gcloud config get-value project\n",
        "# # # Done: Auth + Project/Region set"
      ],
      "id": "T-2fJwLbW_cz"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ve-71_DTW_c0"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a short cell that prints the active project using `gcloud config get-value project` and echoes the `REGION` you set.\n"
      ],
      "id": "ve-71_DTW_c0"
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify the active project and region\n",
        "!gcloud config get-value project\n",
        "import os\n",
        "print(\"REGION:\", os.environ.get(\"REGION\"))"
      ],
      "metadata": {
        "id": "PHYURwA8ZkUn"
      },
      "id": "PHYURwA8ZkUn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5odeke4AW_c0"
      },
      "source": [
        "**Reflection:** Why do we set `PROJECT_ID` and `REGION` at the top? What can go wrong if we don’t?"
      ],
      "id": "5odeke4AW_c0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "This ensures consisteny - all subsequent operations will be performed within the specified project and region. It also meets the dependencies of many Google Cloud commands and helps control costs that can vary by region. If you don't set your projectID and region at the top, commands may defualt to a different project or region and resources may be created in the wrong place. Besides being inconvenient and frustrating, this also impairs reproducibility."
      ],
      "metadata": {
        "id": "gO-trzVYZ5kH"
      },
      "id": "gO-trzVYZ5kH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Z29oLeOW_c1"
      },
      "source": [
        "## 1) Kaggle API — What & Why\n",
        "Use Kaggle CLI for reproducible downloads. Store `kaggle.json` at `~/.kaggle/kaggle.json` with `0600` permissions to protect secrets."
      ],
      "id": "7Z29oLeOW_c1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gxl8m6WWW_c1"
      },
      "source": [
        "### Build Prompt\n",
        "Generate a **single Colab code cell** that:\n",
        "- Prompts me to upload `kaggle.json`,\n",
        "- Saves to `~/.kaggle/kaggle.json` with `0600` permissions,\n",
        "- Prints `kaggle --version`.\n",
        "Add comments about security and reproducibility.\n"
      ],
      "id": "Gxl8m6WWW_c1"
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt the user to upload their Kaggle API token (kaggle.json)\n",
        "# This file contains your Kaggle credentials and should be kept secure.\n",
        "from google.colab import files\n",
        "print(\"Upload your kaggle.json (Go to Kaggle -> Account -> Create New API Token)\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Save the uploaded file to the correct directory with secure permissions\n",
        "# The .kaggle directory in the home folder is the default location for Kaggle config.\n",
        "# Setting permissions to 0600 ensures only the file owner can read and write it,\n",
        "# protecting your credentials from unauthorized access.\n",
        "import os\n",
        "os.makedirs('/root/.kaggle', exist_ok=True) # Create the directory if it doesn't exist\n",
        "with open('/root/.kaggle/kaggle.json', 'wb') as f:\n",
        "    f.write(uploaded[list(uploaded.keys())[0]])\n",
        "os.chmod('/root/.kaggle/kaggle.json', 0o600)  # Set owner-only read/write permissions\n",
        "\n",
        "# Verify the Kaggle CLI is installed and configured correctly by printing its version.\n",
        "# This step helps ensure that subsequent Kaggle commands will work as expected,\n",
        "# contributing to the reproducibility of the notebook.\n",
        "!kaggle --version"
      ],
      "metadata": {
        "id": "Vj88cGoYauhK"
      },
      "id": "Vj88cGoYauhK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FH3yIdyFW_c1"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — Kaggle setup (commented)\n",
        "# # from google.colab import files\n",
        "# # print(\"Upload your kaggle.json (Kaggle > Account > Create New API Token)\")\n",
        "# # uploaded = files.upload()\n",
        "# #\n",
        "# # import os\n",
        "# # os.makedirs('/root/.kaggle', exist_ok=True)\n",
        "# # with open('/root/.kaggle/kaggle.json', 'wb') as f:\n",
        "# #     f.write(uploaded[list(uploaded.keys())[0]])\n",
        "# # os.chmod('/root/.kaggle/kaggle.json', 0o600)  # owner-only\n",
        "# #\n",
        "# # !kaggle --version"
      ],
      "id": "FH3yIdyFW_c1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2ez5mhGW_c2"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a one-liner that runs `kaggle --help | head -n 20` to show the CLI is ready.\n"
      ],
      "id": "b2ez5mhGW_c2"
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify the Kaggle CLI is ready\n",
        "!kaggle --help | head -n 20"
      ],
      "metadata": {
        "id": "EHL5XW5bdyOx"
      },
      "id": "EHL5XW5bdyOx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKab0HxQW_c2"
      },
      "source": [
        "**Reflection:** Why require strict `0600` permissions on API tokens? What risks are we avoiding?"
      ],
      "id": "oKab0HxQW_c2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "0600 permissions allow only the file's owner to read and write to the file. If the file is available to other users, it is possible that they could use it to gain access to private information in my Kaggle account. These permissions protect me and my work."
      ],
      "metadata": {
        "id": "YaRncjeoMBW_"
      },
      "id": "YaRncjeoMBW_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7npSqNHW_c2"
      },
      "source": [
        "## 2) Download & unzip dataset — What & Why\n",
        "Keep raw files under `/content/data/raw` for predictable paths and auditing.\n",
        "**Dataset:** `sayeeduddin/netflix-2025user-behavior-dataset-210k-records`"
      ],
      "id": "v7npSqNHW_c2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuSr1v0TW_c3"
      },
      "source": [
        "### Build Prompt\n",
        "Generate a **Colab code cell** that:\n",
        "- Creates `/content/data/raw`,\n",
        "- Downloads the dataset to `/content/data` with Kaggle CLI,\n",
        "- Unzips into `/content/data/raw` (overwrite OK),\n",
        "- Lists all CSVs with sizes in a neat table.\n",
        "Include comments describing each step.\n"
      ],
      "id": "DuSr1v0TW_c3"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the directory to store raw data\n",
        "# -p flag ensures parent directories are created if they don't exist\n",
        "# /content/data/raw is a standard location for raw data in Colab\n",
        "!mkdir -p /content/data/raw\n",
        "\n",
        "# Download the dataset from Kaggle\n",
        "# -d specifies the dataset identifier (owner/dataset-name)\n",
        "# -p specifies the path where the dataset zip file will be downloaded\n",
        "# The Kaggle CLI uses the credentials configured in ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d sayeeduddin/netflix-2025user-behavior-dataset-210k-records -p /content/data\n",
        "\n",
        "# Unzip the downloaded dataset into the raw data directory\n",
        "# -o flag allows overwriting existing files\n",
        "# *.zip matches the downloaded zip file in /content/data\n",
        "# -d specifies the destination directory for unzipping\n",
        "!unzip -o /content/data/*.zip -d /content/data/raw\n",
        "\n",
        "# List all CSV files in the raw data directory with their sizes\n",
        "# -l uses a long listing format\n",
        "# -h prints file sizes in human-readable format (e.g., KB, MB)\n",
        "# *.csv matches all files with a .csv extension\n",
        "!ls -lh /content/data/raw/*.csv"
      ],
      "metadata": {
        "id": "NJ49H3oLekIQ"
      },
      "id": "NJ49H3oLekIQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3d7OJknW_c3"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — Download & unzip (commented)\n",
        "# # !mkdir -p /content/data/raw\n",
        "# # !kaggle datasets download -d sayeeduddin/netflix-2025user-behavior-dataset-210k-records -p /content/data\n",
        "# # !unzip -o /content/data/*.zip -d /content/data/raw\n",
        "# # # List CSV inventory\n",
        "# # !ls -lh /content/data/raw/*.csv"
      ],
      "id": "B3d7OJknW_c3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxB_jFnNW_c3"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a snippet that asserts there are exactly **six** CSV files and prints their names.\n"
      ],
      "id": "sxB_jFnNW_c3"
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify the number of CSV files and print their names\n",
        "import glob\n",
        "csv_files = glob.glob('/content/data/raw/*.csv')\n",
        "print(\"Found\", len(csv_files), \"CSV files:\")\n",
        "for csv_file in csv_files:\n",
        "    print(csv_file)\n",
        "assert len(csv_files) == 6, f\"Expected 6 CSV files, but found {len(csv_files)}\""
      ],
      "metadata": {
        "id": "hr6heCBVe5zY"
      },
      "id": "hr6heCBVe5zY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mzFxaFJW_c3"
      },
      "source": [
        "**Reflection:** Why is keeping a clean file inventory (names, sizes) useful downstream?"
      ],
      "id": "3mzFxaFJW_c3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clean file inventory allows easier data manipulation and reproducibility. It also makes troubleshooting, automating, and scripting easier."
      ],
      "metadata": {
        "id": "9cvicIRjfK8L"
      },
      "id": "9cvicIRjfK8L"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_H6LNIsjW_c3"
      },
      "source": [
        "## 3) Create GCS bucket & upload — What & Why\n",
        "Stage in GCS → consistent, versionable source for BigQuery loads. Bucket names must be **globally unique**."
      ],
      "id": "_H6LNIsjW_c3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5IzfeHrW_c3"
      },
      "source": [
        "### Build Prompt\n",
        "Generate a **Colab code cell** that:\n",
        "- Creates a unique bucket in `${REGION}` (random suffix),\n",
        "- Saves name to `BUCKET_NAME` env var,\n",
        "- Uploads all CSVs to `gs://$BUCKET_NAME/netflix/`,\n",
        "- Prints the bucket name and explains staging benefits.\n"
      ],
      "id": "i5IzfeHrW_c3"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a unique GCS bucket and upload the raw data\n",
        "import uuid, os\n",
        "\n",
        "# Generate a unique bucket name with a random suffix\n",
        "# GCS bucket names must be globally unique. Using a random suffix helps avoid naming conflicts.\n",
        "bucket_name = f\"mgmt467-netflix-{uuid.uuid4().hex[:8]}\"\n",
        "os.environ[\"BUCKET_NAME\"] = bucket_name\n",
        "\n",
        "# Create the GCS bucket\n",
        "# --location=$REGION specifies the region for the bucket, which we set earlier.\n",
        "# This command will fail if the bucket name is not globally unique, but the random suffix\n",
        "# makes this unlikely.\n",
        "!gcloud storage buckets create gs://$BUCKET_NAME --location=$REGION\n",
        "\n",
        "# Upload all CSV files from the raw data directory to the bucket\n",
        "# gs://$BUCKET_NAME/netflix/ creates a 'netflix' folder within the bucket\n",
        "!gcloud storage cp /content/data/raw/* gs://$BUCKET_NAME/netflix/\n",
        "\n",
        "# Print the bucket name for verification and future reference\n",
        "print(f\"Created and uploaded data to GCS bucket: {bucket_name}\")\n",
        "\n",
        "# Explain the benefits of staging data in GCS\n",
        "print(\"\\nBenefits of staging data in GCS:\")\n",
        "print(\"- Consistent and versionable source for BigQuery loads.\")\n",
        "print(\"- Decouples data source from BigQuery for flexible processing.\")\n",
        "print(\"- Leverages GCS's scalability, reliability, and integration with GCP services.\")"
      ],
      "metadata": {
        "id": "eqWipjy4gE_O"
      },
      "id": "eqWipjy4gE_O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDk9BzDIW_c3"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — GCS staging (commented)\n",
        "# # import uuid, os\n",
        "# # bucket_name = f\"mgmt467-netflix-{uuid.uuid4().hex[:8]}\"\n",
        "# # os.environ[\"BUCKET_NAME\"] = bucket_name\n",
        "# # !gcloud storage buckets create gs://$BUCKET_NAME --location=$REGION\n",
        "# # !gcloud storage cp /content/data/raw/* gs://$BUCKET_NAME/netflix/\n",
        "# # print(\"Bucket:\", bucket_name)\n",
        "# # # Verify contents\n",
        "# # !gcloud storage ls gs://$BUCKET_NAME/netflix/"
      ],
      "id": "GDk9BzDIW_c3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XslCBcGW_c4"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a snippet that lists the `netflix/` prefix and shows object sizes.\n"
      ],
      "id": "0XslCBcGW_c4"
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify the contents of the GCS bucket and show object sizes\n",
        "# -l flag provides a long listing format, including size\n",
        "# gs://$BUCKET_NAME/netflix/ specifies the path to list within the bucket\n",
        "import os\n",
        "!gcloud storage ls -l gs://$BUCKET_NAME/netflix/"
      ],
      "metadata": {
        "id": "eRcKOM9WgjKe"
      },
      "id": "eRcKOM9WgjKe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svCakPJeW_c4"
      },
      "source": [
        "**Reflection:** Name two benefits of staging in GCS vs loading directly from local Colab."
      ],
      "id": "svCakPJeW_c4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Staging the dataset in GCS creates a stable, traceable source for your data. This enables troubeshooting, reproducibility, and understandability. It also improves scalability and ensures your data persists even when the Colab session ends."
      ],
      "metadata": {
        "id": "UbCFTmq7gsrn"
      },
      "id": "UbCFTmq7gsrn"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XqGKgRSW_c4"
      },
      "source": [
        "## 4) BigQuery dataset & loads — What & Why\n",
        "Create dataset `netflix` and load six CSVs with **autodetect** for speed (we’ll enforce schemas later)."
      ],
      "id": "-XqGKgRSW_c4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5lSi7h0W_c4"
      },
      "source": [
        "### Build Prompt (two cells)\n",
        "**Cell A:** Create (idempotently) dataset `netflix` in US multi-region; if it exists, print a friendly message.  \n",
        "**Cell B:** Load tables from `gs://$BUCKET_NAME/netflix/`:\n",
        "`users, movies, watch_history, recommendation_logs, search_logs, reviews`\n",
        "with `--skip_leading_rows=1 --autodetect --source_format=CSV`.\n",
        "Finish with row-count queries for each table.\n"
      ],
      "id": "i5lSi7h0W_c4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZZJEepJW_c4"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — BigQuery dataset (commented)\n",
        "# # DATASET=\"netflix\"\n",
        "# # # Attempt to create; ignore if exists\n",
        "# # !bq --location=US mk -d --description \"MGMT467 Netflix dataset\" $DATASET || echo \"Dataset may already exist.\""
      ],
      "id": "jZZJEepJW_c4"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the BigQuery dataset if it doesn't exist\n",
        "# bq mk -d creates a dataset\n",
        "# --location=US specifies the multi-region location\n",
        "# --description adds a description to the dataset\n",
        "# || echo \"Dataset may already exist.\" makes the command idempotent by\n",
        "# suppressing the error if the dataset already exists and printing a friendly message instead.\n",
        "DATASET = \"netflix\"\n",
        "!bq --location=US mk -d --description \"MGMT467 Netflix dataset\" $DATASET || echo \"Dataset may already exist.\""
      ],
      "metadata": {
        "id": "UEdisPW4hrDj"
      },
      "id": "UEdisPW4hrDj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8TWc5RfW_c4"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — Load tables (commented)\n",
        "# # tables = {\n",
        "# #   \"users\": \"users.csv\",\n",
        "# #   \"movies\": \"movies.csv\",\n",
        "# #   \"watch_history\": \"watch_history.csv\",\n",
        "# #   \"recommendation_logs\": \"recommendation_logs.csv\",\n",
        "# #   \"search_logs\": \"search_logs.csv\",\n",
        "# #   \"reviews\": \"reviews.csv\",\n",
        "# # }\n",
        "# # import os\n",
        "# # for tbl, fname in tables.items():\n",
        "# #   src = f\"gs://{os.environ['BUCKET_NAME']}/netflix/{fname}\"\n",
        "# #   print(\"Loading\", tbl, \"from\", src)\n",
        "# #   !bq load --skip_leading_rows=1 --autodetect --source_format=CSV $DATASET.$tbl $src\n",
        "# #\n",
        "# # # Row counts\n",
        "# # for tbl in tables.keys():\n",
        "# #   !bq query --nouse_legacy_sql \"SELECT '{tbl}' AS table_name, COUNT(*) AS n FROM `${GOOGLE_CLOUD_PROJECT}.netflix.{tbl}`\".format(tbl=tbl)"
      ],
      "id": "C8TWc5RfW_c4"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tables from GCS into BigQuery\n",
        "# --skip_leading_rows=1 skips the header row in the CSV files\n",
        "# --autodetect allows BigQuery to automatically determine the schema\n",
        "# --source_format=CSV specifies that the source files are in CSV format\n",
        "# $DATASET.$tbl specifies the destination table in BigQuery\n",
        "# $src specifies the source file in GCS\n",
        "tables = {\n",
        "  \"users\": \"users.csv\",\n",
        "  \"movies\": \"movies.csv\",\n",
        "  \"watch_history\": \"watch_history.csv\",\n",
        "  \"recommendation_logs\": \"recommendation_logs.csv\",\n",
        "  \"search_logs\": \"search_logs.csv\",\n",
        "  \"reviews\": \"reviews.csv\",\n",
        "}\n",
        "import os\n",
        "for tbl, fname in tables.items():\n",
        "  src = f\"gs://{os.environ['BUCKET_NAME']}/netflix/{fname}\"\n",
        "  print(f\"Loading {tbl} from {src}\")\n",
        "  !bq load --skip_leading_rows=1 --autodetect --source_format=CSV {DATASET}.{tbl} {src}\n",
        "\n",
        "# Query row counts for each table\n",
        "# --nouse_legacy_sql ensures standard SQL is used\n",
        "# SELECT '{tbl}' AS table_name, COUNT(*) AS n FROM `${GOOGLE_CLOUD_PROJECT}.netflix.{tbl}` gets the table name and count\n",
        "\"\"\"\n",
        "print(\"\\nRow counts:\")\n",
        "for tbl in tables.keys():\n",
        "  query = f\"SELECT '{tbl}' AS table_name, COUNT(*) AS n FROM `{os.environ['GOOGLE_CLOUD_PROJECT']}.netflix.{tbl}`\"\n",
        "  !bq query --nouse_legacy_sql \"{query}\"\n",
        "\"\"\""
      ],
      "metadata": {
        "collapsed": true,
        "id": "VINfSqznh2sL"
      },
      "id": "VINfSqznh2sL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhkqFz96W_c4"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a single query that returns `table_name, row_count` for all six tables in `${GOOGLE_CLOUD_PROJECT}.netflix`.\n"
      ],
      "id": "yhkqFz96W_c4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGFc7_iFiTkc",
        "collapsed": true
      },
      "source": [
        "%%bigquery --project directed-bongo-471119-d1\n",
        "SELECT 'users' AS table_name, COUNT(*) AS row_count FROM `directed-bongo-471119-d1.netflix.users`\n",
        "UNION ALL\n",
        "SELECT 'movies' AS table_name, COUNT(*) AS row_count FROM `directed-bongo-471119-d1.netflix.movies`\n",
        "UNION ALL\n",
        "SELECT 'watch_history' AS table_name, COUNT(*) AS row_count FROM `directed-bongo-471119-d1.netflix.watch_history`\n",
        "UNION ALL\n",
        "SELECT 'recommendation_logs' AS table_name, COUNT(*) AS row_count FROM `directed-bongo-471119-d1.netflix.recommendation_logs`\n",
        "UNION ALL\n",
        "SELECT 'search_logs' AS table_name, COUNT(*) AS row_count FROM `directed-bongo-471119-d1.netflix.search_logs`\n",
        "UNION ALL\n",
        "SELECT 'reviews' AS table_name, COUNT(*) AS row_count FROM `directed-bongo-471119-d1.netflix.reviews`"
      ],
      "id": "uGFc7_iFiTkc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDJ-7JHcW_c5"
      },
      "source": [
        "**Reflection:** When is `autodetect` acceptable? When should you enforce explicit schemas and why?"
      ],
      "id": "MDJ-7JHcW_c5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "autodetect is acceptable for exploratory data analysis, simple schemas, or rapid prototyping. Explicit schemas should be used for production pipelines and with complex data types to ensure data quality and consistency. It also creates clear doucumentation of your data's structure."
      ],
      "metadata": {
        "id": "h6uJdI_miUk7"
      },
      "id": "h6uJdI_miUk7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcbke_06W_c5"
      },
      "source": [
        "## 5) Data Quality (DQ) — Concepts we care about\n",
        "- **Missingness** (MCAR/MAR/MNAR). Impute vs drop. Add `is_missing_*` indicators.\n",
        "- **Duplicates** (exact vs near). Double-counted engagement corrupts labels & KPIs.\n",
        "- **Outliers** (IQR). Winsorize/cap vs robust models. Always **flag** and explain.\n",
        "- **Reproducibility**. Prefer `CREATE OR REPLACE` and deterministic keys.\n"
      ],
      "id": "mcbke_06W_c5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkMhI_t3W_c5"
      },
      "source": [
        "### 5.1 Missingness (users) — What & Why\n",
        "Measure % missing and check if missingness depends on another variable (MAR) → potential bias & instability."
      ],
      "id": "OkMhI_t3W_c5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q648UiSbW_c5"
      },
      "source": [
        "### Build Prompt\n",
        "Generate **two BigQuery SQL cells**:\n",
        "1) Total rows and % missing in `region`, `plan_tier`, `age_band` from `users`.\n",
        "2) `% plan_tier missing by region` ordered descending. Add comments on MAR.\n"
      ],
      "id": "Q648UiSbW_c5"
    },
    {
      "cell_type": "code",
      "source": [
        "%%bigquery --project directed-bongo-471119-d1\n",
        "-- Users: % missing per column\n",
        "WITH base AS (\n",
        "  SELECT COUNT(*) n,\n",
        "         COUNTIF(state_province IS NULL) miss_region,\n",
        "         COUNTIF(subscription_plan IS NULL) miss_plan,\n",
        "         COUNTIF(age IS NULL) miss_age\n",
        "  FROM `directed-bongo-471119-d1.netflix.users`\n",
        ")\n",
        "SELECT n,\n",
        "       ROUND(100*miss_region/n,2) AS pct_missing_region,\n",
        "       ROUND(100*miss_plan/n,2)   AS pct_missing_plan_tier,\n",
        "       ROUND(100*miss_age/n,2)    AS pct_missing_age_band\n",
        "FROM base;"
      ],
      "metadata": {
        "collapsed": true,
        "id": "kVA2A-MGPB2k"
      },
      "id": "kVA2A-MGPB2k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bigquery --project directed-bongo-471119-d1\n",
        "-- Calculate the percentage of missing subscription_plan by state_province\n",
        "SELECT\n",
        "    state_province,\n",
        "    COUNT(*) AS n,\n",
        "    ROUND(100.0 * COUNTIF(subscription_plan IS NULL) / COUNT(*), 2) AS pct_missing_subscription_plan\n",
        "FROM\n",
        "    `directed-bongo-471119-d1.netflix.users`\n",
        "GROUP BY\n",
        "    state_province\n",
        "ORDER BY\n",
        "    pct_missing_subscription_plan DESC\n",
        "-- Missing at Random (MAR) occurs if the missingness of subscription_plan\n",
        "-- is related to the state_province, but not to the value of subscription_plan itself.\n",
        "-- Analyzing missingness by state_province helps identify potential MAR patterns."
      ],
      "metadata": {
        "collapsed": true,
        "id": "Ec-i_AmpURNa"
      },
      "id": "Ec-i_AmpURNa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7Wa56opW_c5"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — Missingness profile (commented)\n",
        "# # -- Users: % missing per column\n",
        "# # WITH base AS (\n",
        "# #   SELECT COUNT(*) n,\n",
        "# #          COUNTIF(region IS NULL) miss_region,\n",
        "# #          COUNTIF(plan_tier IS NULL) miss_plan,\n",
        "# #          COUNTIF(age_band IS NULL) miss_age\n",
        "# #   FROM `${GOOGLE_CLOUD_PROJECT}.netflix.users`\n",
        "# # )\n",
        "# # SELECT n,\n",
        "# #        ROUND(100*miss_region/n,2) AS pct_missing_region,\n",
        "# #        ROUND(100*miss_plan/n,2)   AS pct_missing_plan_tier,\n",
        "# #        ROUND(100*miss_age/n,2)    AS pct_missing_age_band\n",
        "# # FROM base;"
      ],
      "id": "E7Wa56opW_c5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbXwrUrpW_c6"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — MAR by region (commented)\n",
        "# # SELECT region,\n",
        "# #        COUNT(*) AS n,\n",
        "# #        ROUND(100*COUNTIF(plan_tier IS NULL)/COUNT(*),2) AS pct_missing_plan_tier\n",
        "# # FROM `${GOOGLE_CLOUD_PROJECT}.netflix.users`\n",
        "# # GROUP BY region\n",
        "# # ORDER BY pct_missing_plan_tier DESC;"
      ],
      "id": "dbXwrUrpW_c6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQaG879oW_c6"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a query that prints the three missingness percentages from (1), rounded to two decimals.\n"
      ],
      "id": "YQaG879oW_c6"
    },
    {
      "cell_type": "code",
      "source": [
        "%%bigquery --project directed-bongo-471119-d1\n",
        "-- Combine the missingness percentages from previous queries\n",
        "SELECT 'pct_missing_region' AS metric, pct_missing_region AS percentage FROM (\n",
        "  SELECT ROUND(100*COUNTIF(state_province IS NULL)/COUNT(*),2) AS pct_missing_region\n",
        "  FROM `directed-bongo-471119-d1.netflix.users`\n",
        ")\n",
        "UNION ALL\n",
        "SELECT 'pct_missing_plan_tier' AS metric, pct_missing_plan_tier AS percentage FROM (\n",
        "  SELECT ROUND(100*COUNTIF(subscription_plan IS NULL)/COUNT(*),2) AS pct_missing_plan_tier\n",
        "  FROM `directed-bongo-471119-d1.netflix.users`\n",
        ")\n",
        "UNION ALL\n",
        "SELECT 'pct_missing_age_band' AS metric, pct_missing_age_band AS percentage FROM (\n",
        "  SELECT ROUND(100*COUNTIF(age IS NULL)/COUNT(*),2) AS pct_missing_age_band\n",
        "  FROM `directed-bongo-471119-d1.netflix.users`\n",
        ");"
      ],
      "metadata": {
        "id": "ui62xqvmVDEh"
      },
      "id": "ui62xqvmVDEh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzpbdD2GW_c6"
      },
      "source": [
        "**Reflection:** Which columns are most missing? Hypothesize MCAR/MAR/MNAR and why."
      ],
      "id": "bzpbdD2GW_c6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Age has the most missing values of these columns from the user table. These values could be MCAR if age is not required when creating account and many users don't want to bother to put theirs in or do not feel that it is safe to disclose their birthdate. However, this could also be MNAR if users of a certain age do not want to disclose that information."
      ],
      "metadata": {
        "id": "4lA1JZS3VX53"
      },
      "id": "4lA1JZS3VX53"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WVDAmtiW_c7"
      },
      "source": [
        "### 5.2 Duplicates (watch_history) — What & Why\n",
        "Find exact duplicate interaction records and keep **one best** per group (deterministic policy)."
      ],
      "id": "7WVDAmtiW_c7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvhLIT4hW_c7"
      },
      "source": [
        "### Build Prompt\n",
        "Generate **two BigQuery SQL cells**:\n",
        "1) Report duplicate groups on `(user_id, movie_id, event_ts, device_type)` with counts (top 20).\n",
        "2) Create table `watch_history_dedup` that keeps one row per group (prefer higher `progress_ratio`, then `minutes_watched`). Add comments.\n"
      ],
      "id": "MvhLIT4hW_c7"
    },
    {
      "cell_type": "code",
      "source": [
        "%%bigquery --project directed-bongo-471119-d1\n",
        "-- Report duplicate groups on (user_id, movie_id, action, device_type) with counts (top 20)\n",
        "SELECT user_id, movie_id, action, device_type, COUNT(*) AS dup_count\n",
        "FROM `directed-bongo-471119-d1.netflix.watch_history`\n",
        "GROUP BY user_id, movie_id, action, device_type\n",
        "HAVING dup_count > 1\n",
        "ORDER BY dup_count DESC\n",
        "LIMIT 20;"
      ],
      "metadata": {
        "collapsed": true,
        "id": "IonAbYzUWQTO"
      },
      "id": "IonAbYzUWQTO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bigquery --project directed-bongo-471119-d1\n",
        "-- Create a new table watch_history_dedup by keeping one row per duplicate group\n",
        "-- Duplicates are identified by the combination of user_id, movie_id, action, and device_type.\n",
        "-- The row to keep is selected based on the highest progress_percentage, then the highest watch_duration_minutes.\n",
        "CREATE OR REPLACE TABLE `directed-bongo-471119-d1.netflix.watch_history_dedup` AS\n",
        "SELECT * EXCEPT(rk) FROM (\n",
        "  SELECT h.*,\n",
        "         ROW_NUMBER() OVER (\n",
        "           PARTITION BY user_id, movie_id, action, device_type\n",
        "           ORDER BY progress_percentage DESC, watch_duration_minutes DESC\n",
        "         ) AS rk\n",
        "  FROM `directed-bongo-471119-d1.netflix.watch_history` h\n",
        ")\n",
        "WHERE rk = 1;"
      ],
      "metadata": {
        "id": "EhEHueltXIgA"
      },
      "id": "EhEHueltXIgA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deAep_q1W_c7"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — Detect duplicate groups (commented)\n",
        "# # SELECT user_id, movie_id, event_ts, device_type, COUNT(*) AS dup_count\n",
        "# # FROM `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history`\n",
        "# # GROUP BY user_id, movie_id, event_ts, device_type\n",
        "# # HAVING dup_count > 1\n",
        "# # ORDER BY dup_count DESC\n",
        "# # LIMIT 20;"
      ],
      "id": "deAep_q1W_c7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3kanzsuW_dR"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — Keep-one policy (commented)\n",
        "# # CREATE OR REPLACE TABLE `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history_dedup` AS\n",
        "# # SELECT * EXCEPT(rk) FROM (\n",
        "# #   SELECT h.*,\n",
        "# #          ROW_NUMBER() OVER (\n",
        "# #            PARTITION BY user_id, movie_id, event_ts, device_type\n",
        "# #            ORDER BY progress_ratio DESC, minutes_watched DESC\n",
        "# #          ) AS rk\n",
        "# #   FROM `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history` h\n",
        "# # )\n",
        "# # WHERE rk = 1;"
      ],
      "id": "W3kanzsuW_dR"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3Xu36gDW_dS"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a before/after count query comparing raw vs `watch_history_dedup`.\n"
      ],
      "id": "U3Xu36gDW_dS"
    },
    {
      "cell_type": "code",
      "source": [
        "%%bigquery --project directed-bongo-471119-d1\n",
        "SELECT 'watch_history' AS table_name, COUNT(*) AS row_count FROM `directed-bongo-471119-d1.netflix.watch_history`\n",
        "UNION ALL\n",
        "SELECT 'watch_history_dedup' AS table_name, COUNT(*) AS row_count FROM `directed-bongo-471119-d1.netflix.watch_history_dedup`"
      ],
      "metadata": {
        "id": "-5E_2kdnX_aq"
      },
      "id": "-5E_2kdnX_aq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g251g6QKW_dS"
      },
      "source": [
        "**Reflection:** Why do duplicates arise (natural vs system-generated)? How do they corrupt labels and KPIs?"
      ],
      "id": "g251g6QKW_dS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Duplicates can arise from human error - multiple users imputing the same information to the same table or one user mistakenly entering the same information twice. Errors in data transformation can prompt the system to duplicate data. Parallel processing is the system equivalent of multiple humans entering the same data into a table. Retrying processes that failed midway could also insert duplicate records. They corrupt labels and KPIs because they destroy the accuracy of the information from which they were constructed, causing them to be inaccurate and potentially misleading."
      ],
      "metadata": {
        "id": "KGJAZumxYI3m"
      },
      "id": "KGJAZumxYI3m"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2zFQLj9W_dT"
      },
      "source": [
        "### 5.3 Outliers (minutes_watched) — What & Why\n",
        "Estimate extreme values via IQR; report % outliers; **winsorize** to P01/P99 for robustness while also **flagging** extremes."
      ],
      "id": "h2zFQLj9W_dT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCHd7A2PW_dT"
      },
      "source": [
        "### Build Prompt\n",
        "Generate **two BigQuery SQL cells**:\n",
        "1) Compute IQR bounds for `minutes_watched` on `watch_history_dedup` and report % outliers.\n",
        "2) Create `watch_history_robust` with `minutes_watched_capped` capped at P01/P99; return quantile summaries before/after.\n"
      ],
      "id": "LCHd7A2PW_dT"
    },
    {
      "cell_type": "code",
      "source": [
        "%%bigquery --project directed-bongo-471119-d1\n",
        "-- Compute IQR bounds for watch_duration_minutes and report % outliers\n",
        "WITH dist AS (\n",
        "  SELECT\n",
        "    APPROX_QUANTILES(watch_duration_minutes, 4)[OFFSET(1)] AS q1,\n",
        "    APPROX_QUANTILES(watch_duration_minutes, 4)[OFFSET(3)] AS q3\n",
        "  FROM `directed-bongo-471119-d1.netflix.watch_history_dedup`\n",
        "),\n",
        "bounds AS (\n",
        "  SELECT q1, q3, (q3-q1) AS iqr,\n",
        "         q1 - 1.5*(q3-q1) AS lo,\n",
        "         q3 + 1.5*(q3-1) AS hi -- Corrected: Use q3-q1 for IQR calculation\n",
        "  FROM dist\n",
        ")\n",
        "SELECT\n",
        "  COUNTIF(h.watch_duration_minutes < b.lo OR h.watch_duration_minutes > b.hi) AS outliers,\n",
        "  COUNT(*) AS total,\n",
        "  ROUND(100*COUNTIF(h.watch_duration_minutes < b.lo OR h.watch_duration_minutes > b.hi)/COUNT(*),2) AS pct_outliers\n",
        "FROM `directed-bongo-471119-d1.netflix.watch_history_dedup` h\n",
        "CROSS JOIN bounds b;"
      ],
      "metadata": {
        "id": "QePydK6-Zb9O"
      },
      "id": "QePydK6-Zb9O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bigquery --project directed-bongo-471119-d1\n",
        "-- Create watch_history_robust with minutes_watched_capped at P01/P99\n",
        "CREATE OR REPLACE TABLE `directed-bongo-471119-d1.netflix.watch_history_robust` AS\n",
        "WITH q AS (\n",
        "  SELECT\n",
        "    APPROX_QUANTILES(watch_duration_minutes, 100)[OFFSET(1)]  AS p01,\n",
        "    APPROX_QUANTILES(watch_duration_minutes, 100)[OFFSET(98)] AS p99\n",
        "  FROM `directed-bongo-471119-d1.netflix.watch_history_dedup`\n",
        ")\n",
        "SELECT\n",
        "  h.*,\n",
        "  GREATEST(q.p01, LEAST(q.p99, h.watch_duration_minutes)) AS watch_duration_minutes_capped\n",
        "FROM `directed-bongo-471119-d1.netflix.watch_history_dedup` h, q;\n",
        "\n",
        "-- Quantiles before vs after capping\n",
        "WITH before AS (\n",
        "  SELECT 'before' AS which, APPROX_QUANTILES(watch_duration_minutes, 5) AS q\n",
        "  FROM `directed-bongo-471119-d1.netflix.watch_history_dedup`\n",
        "),\n",
        "after AS (\n",
        "  SELECT 'after' AS which, APPROX_QUANTILES(watch_duration_minutes_capped, 5) AS q\n",
        "  FROM `directed-bongo-471119-d1.netflix.watch_history_robust`\n",
        ")\n",
        "SELECT * FROM before UNION ALL SELECT * FROM after;"
      ],
      "metadata": {
        "id": "2VsrBzqra03f"
      },
      "id": "2VsrBzqra03f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yk_5w1t1W_dT"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — IQR outlier rate (commented)\n",
        "# # WITH dist AS (\n",
        "# #   SELECT\n",
        "# #     APPROX_QUANTILES(minutes_watched, 4)[OFFSET(1)] AS q1,\n",
        "# #     APPROX_QUANTILES(minutes_watched, 4)[OFFSET(3)] AS q3\n",
        "# #   FROM `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history_dedup`\n",
        "# # ),\n",
        "# # bounds AS (\n",
        "# #   SELECT q1, q3, (q3-q1) AS iqr,\n",
        "# #          q1 - 1.5*(q3-q1) AS lo,\n",
        "# #          q3 + 1.5*(q3-q1) AS hi\n",
        "# #   FROM dist\n",
        "# # )\n",
        "# # SELECT\n",
        "# #   COUNTIF(h.minutes_watched < b.lo OR h.minutes_watched > b.hi) AS outliers,\n",
        "# #   COUNT(*) AS total,\n",
        "# #   ROUND(100*COUNTIF(h.minutes_watched < b.lo OR h.minutes_watched > b.hi)/COUNT(*),2) AS pct_outliers\n",
        "# # FROM `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history_dedup` h\n",
        "# # CROSS JOIN bounds b;"
      ],
      "id": "yk_5w1t1W_dT"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBgrRfYFW_dT"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — Winsorize + quantiles (commented)\n",
        "# # CREATE OR REPLACE TABLE `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history_robust` AS\n",
        "# # WITH q AS (\n",
        "# #   SELECT\n",
        "# #     APPROX_QUANTILES(minutes_watched, 100)[OFFSET(1)]  AS p01,\n",
        "# #     APPROX_QUANTILES(minutes_watched, 100)[OFFSET(98)] AS p99\n",
        "# #   FROM `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history_dedup`\n",
        "# # )\n",
        "# # SELECT\n",
        "# #   h.*,\n",
        "# #   GREATEST(q.p01, LEAST(q.p99, h.minutes_watched)) AS minutes_watched_capped\n",
        "# # FROM `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history_dedup` h, q;\n",
        "# #\n",
        "# # -- Quantiles before vs after\n",
        "# # WITH before AS (\n",
        "# #   SELECT 'before' AS which, APPROX_QUANTILES(minutes_watched, 5) AS q\n",
        "# #   FROM `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history_dedup`\n",
        "# # ),\n",
        "# # after AS (\n",
        "# #   SELECT 'after' AS which, APPROX_QUANTILES(minutes_watched_capped, 5) AS q\n",
        "# #   FROM `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history_robust`\n",
        "# # )\n",
        "# # SELECT * FROM before UNION ALL SELECT * FROM after;"
      ],
      "id": "eBgrRfYFW_dT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdOcgZ6yW_dU"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a query that shows min/median/max before vs after capping.\n"
      ],
      "id": "kdOcgZ6yW_dU"
    },
    {
      "cell_type": "code",
      "source": [
        "%%bigquery --project directed-bongo-471119-d1\n",
        "SELECT 'watch_history' AS table_name,\n",
        "       MIN(watch_duration_minutes) AS min_duration,\n",
        "       APPROX_QUANTILES(watch_duration_minutes, 2)[OFFSET(1)] AS median_duration,\n",
        "       MAX(watch_duration_minutes) AS max_duration\n",
        "FROM `directed-bongo-471119-d1.netflix.watch_history_dedup`\n",
        "UNION ALL\n",
        "SELECT 'watch_history_robust' AS table_name,\n",
        "       MIN(watch_duration_minutes_capped) AS min_duration,\n",
        "       APPROX_QUANTILES(watch_duration_minutes_capped, 2)[OFFSET(1)] AS median_duration,\n",
        "       MAX(watch_duration_minutes_capped) AS max_duration\n",
        "FROM `directed-bongo-471119-d1.netflix.watch_history_robust`;"
      ],
      "metadata": {
        "id": "IN8WmAlAb5Y8"
      },
      "id": "IN8WmAlAb5Y8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bv3rg62nW_dU"
      },
      "source": [
        "**Reflection:** When might capping be harmful? Name a model type less sensitive to outliers and why."
      ],
      "id": "Bv3rg62nW_dU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Capping could be harmful if the outlier represents an accurate data point. It can be very valuable to understand how and why some data points contain extreme values. Decision trees are less sensitive to outliers because of the way they split data. Outliers can only affect the tree if they are included in a decision node, and their effect is limited to that node."
      ],
      "metadata": {
        "id": "7kbn5I4zcdUX"
      },
      "id": "7kbn5I4zcdUX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nk9k3jiW_dU"
      },
      "source": [
        "### 5.4 Business anomaly flags — What & Why\n",
        "Human-readable flags help both product decisioning and ML features (e.g., binge behavior)."
      ],
      "id": "8nk9k3jiW_dU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vP6BcSuDW_dU"
      },
      "source": [
        "### Build Prompt\n",
        "Generate **three BigQuery SQL cells** (adjust if columns differ):\n",
        "1) In `watch_history_robust`, compute and summarize `flag_binge` for sessions > 8 hours.\n",
        "2) In `users`, compute and summarize `flag_age_extreme` if age can be parsed from `age_band` (<10 or >100).\n",
        "3) In `movies`, compute and summarize `flag_duration_anomaly` where `duration_min` < 15 or > 480 (if exists).\n",
        "Each cell should output count and percentage and include 1–2 comments.\n"
      ],
      "id": "vP6BcSuDW_dU"
    },
    {
      "cell_type": "code",
      "source": [
        "%%bigquery --project directed-bongo-471119-d1\n",
        "-- Summarize flag_binge for watch sessions > 8 hours (480 minutes)\n",
        "SELECT\n",
        "  COUNTIF(watch_duration_minutes > 480) AS sessions_over_8h,\n",
        "  COUNT(*) AS total,\n",
        "  ROUND(100*COUNTIF(watch_duration_minutes > 480)/COUNT(*),2) AS pct\n",
        "FROM `directed-bongo-471119-d1.netflix.watch_history_robust`;"
      ],
      "metadata": {
        "id": "nLvqsZ0YeVnE"
      },
      "id": "nLvqsZ0YeVnE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bigquery --project directed-bongo-471119-d1\n",
        "-- Summarize flag_age_extreme if age of user is less than 10 or greater than 100\n",
        "SELECT\n",
        "  COUNTIF(age < 10 OR age > 100) AS extreme_age_rows,\n",
        "  COUNT(*) AS total,\n",
        "  ROUND(100*COUNTIF(age < 10 OR age > 100)/COUNT(*),2) AS pct\n",
        "FROM `directed-bongo-471119-d1.netflix.users`;"
      ],
      "metadata": {
        "id": "4oh80Slie4-k"
      },
      "id": "4oh80Slie4-k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bigquery --project directed-bongo-471119-d1\n",
        "-- Summarize flag_duration_anomaly for movies where duration_min < 15 or > 480\n",
        "SELECT\n",
        "  COUNTIF(duration_minutes < 15) AS titles_under_15m,\n",
        "  COUNTIF(duration_minutes > 480) AS titles_over_8h,\n",
        "  COUNT(*) AS total,\n",
        "  ROUND(100 * (COUNTIF(duration_minutes < 15) + COUNTIF(duration_minutes > 480)) / COUNT(*), 2) AS pct_duration_anomaly\n",
        "FROM `directed-bongo-471119-d1.netflix.movies`;"
      ],
      "metadata": {
        "id": "3TFARfROfUH6"
      },
      "id": "3TFARfROfUH6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3C-xcwjzW_dV"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — flag_binge (commented)\n",
        "# # SELECT\n",
        "# #   COUNTIF(minutes_watched > 8*60) AS sessions_over_8h,\n",
        "# #   COUNT(*) AS total,\n",
        "# #   ROUND(100*COUNTIF(minutes_watched > 8*60)/COUNT(*),2) AS pct\n",
        "# # FROM `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history_robust`;"
      ],
      "id": "3C-xcwjzW_dV"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QOccz8CW_dV"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — flag_age_extreme (commented)\n",
        "# # SELECT\n",
        "# #   COUNTIF(CAST(REGEXP_EXTRACT(age_band, r'\\d+') AS INT64) < 10 OR\n",
        "# #           CAST(REGEXP_EXTRACT(age_band, r'\\d+') AS INT64) > 100) AS extreme_age_rows,\n",
        "# #   COUNT(*) AS total,\n",
        "# #   ROUND(100*COUNTIF(CAST(REGEXP_EXTRACT(age_band, r'\\d+') AS INT64) < 10 OR\n",
        "# #                     CAST(REGEXP_EXTRACT(age_band, r'\\d+') AS INT64) > 100)/COUNT(*),2) AS pct\n",
        "# # FROM `${GOOGLE_CLOUD_PROJECT}.netflix.users`;"
      ],
      "id": "4QOccz8CW_dV"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwQm2v7CW_dV"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — flag_duration_anomaly (commented)\n",
        "# # SELECT\n",
        "# #   COUNTIF(duration_min < 15) AS titles_under_15m,\n",
        "# #   COUNTIF(duration_min > 8*60) AS titles_over_8h,\n",
        "# #   COUNT(*) AS total\n",
        "# # FROM `${GOOGLE_CLOUD_PROJECT}.netflix.movies`;"
      ],
      "id": "OwQm2v7CW_dV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVBJZqUKW_dV"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a single compact summary query that returns two columns per flag: `flag_name, pct_of_rows`.\n"
      ],
      "id": "yVBJZqUKW_dV"
    },
    {
      "cell_type": "code",
      "source": [
        "%%bigquery --project directed-bongo-471119-d1\n",
        "-- Summarize all anomaly flags in a single query\n",
        "SELECT 'flag_binge' AS flag_name, ROUND(100*COUNTIF(watch_duration_minutes > 480)/COUNT(*),2) AS pct_of_rows\n",
        "FROM `directed-bongo-471119-d1.netflix.watch_history_robust`\n",
        "UNION ALL\n",
        "SELECT 'flag_age_extreme' AS flag_name, ROUND(100*COUNTIF(age < 10 OR age > 100)/COUNT(*),2) AS pct_of_rows\n",
        "FROM `directed-bongo-471119-d1.netflix.users`\n",
        "UNION ALL\n",
        "SELECT 'flag_duration_anomaly' AS flag_name, ROUND(100 * (COUNTIF(duration_minutes < 15) + COUNTIF(duration_minutes > 480)) / COUNT(*), 2) AS pct_of_rows\n",
        "FROM `directed-bongo-471119-d1.netflix.movies`;"
      ],
      "metadata": {
        "id": "UNb61HTrfu8R"
      },
      "id": "UNb61HTrfu8R",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diwzO_m3W_dW"
      },
      "source": [
        "**Reflection:** Which anomaly flag is most common? Which would you keep as a feature and why?"
      ],
      "id": "diwzO_m3W_dW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The duration anomaly flag for movies shorter than 15m or longer than 8 hours is the most common. I would keep the binge flag. I think there could be valuable insights gained from understanding which categories of users binge and what kinds of content are being binged. The age of a user or the duration of some content is less valuable to me."
      ],
      "metadata": {
        "id": "Rc3c8xf6f-3S"
      },
      "id": "Rc3c8xf6f-3S"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-hN5O0PW_dW"
      },
      "source": [
        "## 6) Save & submit — What & Why\n",
        "Reproducibility: save artifacts and document decisions so others can rerun and audit."
      ],
      "id": "Z-hN5O0PW_dW"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYIGlKlUW_dW"
      },
      "source": [
        "### Build Prompt\n",
        "Generate a checklist (Markdown) students can paste at the end:\n",
        "- Save this notebook to the team Drive.\n",
        "- Export a `.sql` file with your DQ queries and save to repo.\n",
        "- Push notebook + SQL to the **team GitHub** with a descriptive commit.\n",
        "- Add a README with your `PROJECT_ID`, `REGION`, bucket, dataset, and today’s row counts.\n"
      ],
      "id": "cYIGlKlUW_dW"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwkLRAOYW_dW"
      },
      "source": [
        "## Grading rubric (quick)\n",
        "- Profiling completeness (30)  \n",
        "- Cleaning policy correctness & reproducibility (40)  \n",
        "- Reflection/insight (20)  \n",
        "- Hygiene (naming, verification, idempotence) (10)\n"
      ],
      "id": "rwkLRAOYW_dW"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}